Here's a complete implementation of the GoogleAINativeService class that uses the native Google GenAI SDK:

```python
# swissknife/modules/google/native_service.py
import os
import json
import base64
import mimetypes
from typing import Dict, Any, List, Optional, Tuple
from dotenv import load_dotenv
from google import genai
from google.genai import types
from swissknife.modules.llm.base import BaseLLMService
from swissknife.modules.llm.message import MessageTransformer
from swissknife.modules.prompts.constants import (
    EXPLAIN_PROMPT,
    SUMMARIZE_PROMPT,
    CHAT_SYSTEM_PROMPT,
)


class GoogleAINativeService(BaseLLMService):
    """
    Google GenAI service implementation using the native Python SDK.
    This service connects to Google's Gemini models using the official Google GenAI SDK.
    """

    def __init__(self):
        """Initialize the Google GenAI service."""
        load_dotenv()
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found in environment variables")
        
        # Initialize the Google GenAI client
        self.client = genai.Client(api_key=api_key)
        
        # Default model
        self.model = "gemini-2.0-flash"
        
        # Initialize tools and handlers
        self.tools = []
        self.tool_handlers = {}
        self.tool_definitions = []  # Keep original definitions for reference
        
        # Provider name and system prompt
        self._provider_name = "google_native"
        self.system_prompt = CHAT_SYSTEM_PROMPT

    def set_think(self, budget_tokens: int) -> bool:
        """
        Enable or disable thinking mode with the specified token budget.
        Currently not supported in Google GenAI.

        Args:
            budget_tokens (int): Token budget for thinking. 0 to disable thinking mode.

        Returns:
            bool: True if thinking mode is supported and successfully set, False otherwise.
        """
        print("Thinking mode is not supported for Google GenAI models.")
        return False

    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """
        Calculate the cost based on token usage.

        Args:
            input_tokens (int): Number of input tokens
            output_tokens (int): Number of output tokens

        Returns:
            float: Estimated cost in USD
        """
        # Current Gemini pricing as of March 2025
        input_rate = 0.00000025  # $0.00025 per 1000 input tokens
        output_rate = 0.0000005  # $0.0005 per 1000 output tokens
        
        cost = (input_tokens * input_rate) + (output_tokens * output_rate)
        return cost

    def _process_content(self, prompt_template, content, max_tokens=2048):
        """
        Process content with a given prompt template.

        Args:
            prompt_template (str): The template with {content} placeholder
            content (str): The content to process
            max_tokens (int): Maximum tokens for the response

        Returns:
            str: The processed content
        """
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt_template.format(content=content),
                config=types.GenerateContentConfig(
                    max_output_tokens=max_tokens,
                    temperature=0.2
                )
            )

            # Get token usage if available
            input_tokens = 0
            output_tokens = 0
            if hasattr(response, "usage") and response.usage:
                if hasattr(response.usage, "prompt_tokens"):
                    input_tokens = response.usage.prompt_tokens
                if hasattr(response.usage, "completion_tokens"):
                    output_tokens = response.usage.completion_tokens

            # Calculate and log cost
            total_cost = self.calculate_cost(input_tokens, output_tokens)
            print("\nToken Usage Statistics:")
            print(f"Input tokens: {input_tokens:,}")
            print(f"Output tokens: {output_tokens:,}")
            print(f"Total tokens: {input_tokens + output_tokens:,}")
            print(f"Estimated cost: ${total_cost:.4f}")

            return response.text
        except Exception as e:
            raise Exception(f"Failed to process content: {str(e)}")

    def summarize_content(self, content: str) -> str:
        """
        Summarize the provided content using Google GenAI.

        Args:
            content (str): The content to summarize

        Returns:
            str: The summarized content
        """
        return self._process_content(SUMMARIZE_PROMPT, content, max_tokens=2048)

    def explain_content(self, content: str) -> str:
        """
        Explain the provided content using Google GenAI.

        Args:
            content (str): The content to explain

        Returns:
            str: The explained content
        """
        return self._process_content(EXPLAIN_PROMPT, content, max_tokens=1500)

    def process_file_for_message(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        Process a file and return the appropriate message content.

        Args:
            file_path (str): Path to the file

        Returns:
            Optional[Dict[str, Any]]: The message content or None if processing failed
        """
        try:
            mime_type, _ = mimetypes.guess_type(file_path)
            if not mime_type:
                mime_type = "application/octet-stream"
            
            # For text files, read directly
            if mime_type.startswith("text/"):
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                print(f"ðŸ“„ Including text file: {file_path}")
                return {
                    "type": "text",
                    "text": f"Content of {file_path}:\n\n{content}",
                }
            
            # For binary files, encode as base64
            with open(file_path, "rb") as f:
                content = f.read()
            encoded = base64.b64encode(content).decode("utf-8")
            
            print(f"ðŸ“„ Including file: {file_path} (MIME type: {mime_type})")
            return {
                "mime_type": mime_type,
                "data": encoded,
                "file_name": os.path.basename(file_path)
            }
        except Exception as e:
            print(f"âŒ Error processing file {file_path}: {str(e)}")
            return None

    def handle_file_command(self, file_path: str) -> Optional[List[Dict[str, Any]]]:
        """
        Handle the /file command and return message content.

        Args:
            file_path (str): Path to the file

        Returns:
            Optional[List[Dict[str, Any]]]: Message content or None if processing failed
        """
        result = self.process_file_for_message(file_path)
        if result:
            if "type" in result and result["type"] == "text":
                return [
                    {
                        "type": "text",
                        "text": f"I'm sharing this file with you:\n\n{result['text']}"
                    }
                ]
            else:
                # Upload the file using the Google GenAI SDK's file API
                try:
                    uploaded_file = self.client.files.upload(path=file_path)
                    return [
                        {
                            "type": "text",
                            "text": f"I'm sharing this file with you: {os.path.basename(file_path)}"
                        }
                    ]
                except Exception as e:
                    print(f"âŒ Error uploading file to Google GenAI: {str(e)}")
                    return None
        return None

    def register_tool(self, tool_definition, handler_function):
        """
        Register a tool with its handler function.

        Args:
            tool_definition (dict): The tool definition following OpenAI's function schema
            handler_function (callable): Function to call when tool is used
        """
        # Store original tool definition for reference
        self.tool_definitions.append(tool_definition)
        
        # Extract tool name from definition
        tool_name = self._extract_tool_name(tool_definition)
        
        # Extract parameters schema
        parameters = {}
        required = []
        
        if "function" in tool_definition:
            parameters = tool_definition["function"].get("parameters", {}).get("properties", {})
            required = tool_definition["function"].get("parameters", {}).get("required", [])
            description = tool_definition["function"].get("description", "")
        else:
            parameters = tool_definition.get("parameters", {}).get("properties", {})
            required = tool_definition.get("parameters", {}).get("required", [])
            description = tool_definition.get("description", "")
        
        # Create a function declaration for Google GenAI
        function_declaration = {
            "name": tool_name,
            "description": description,
            "parameters": {
                "type": "OBJECT",
                "properties": {}
            }
        }
        
        # Convert parameters to Google GenAI format
        for param_name, param_def in parameters.items():
            param_type = param_def.get("type", "STRING").upper()
            if param_type == "INTEGER":
                param_type = "NUMBER"
            
            function_declaration["parameters"]["properties"][param_name] = {
                "type": param_type,
                "description": param_def.get("description", "")
            }
        
        # Add required parameters
        if required:
            function_declaration["parameters"]["required"] = required
        
        # Create a Tool object with the function declaration
        self.tools.append(
            types.Tool(function_declarations=[function_declaration])
        )
        
        # Store the handler function
        self.tool_handlers[tool_name] = handler_function
        
        print(f"ðŸ”§ Registered tool: {tool_name}")

    def execute_tool(self, tool_name, tool_params) -> Any:
        """
        Execute a registered tool with the given parameters.

        Args:
            tool_name (str): Name of the tool to execute
            tool_params (dict): Parameters to pass to the tool

        Returns:
            Any: Result of the tool execution
        """
        if tool_name not in self.tool_handlers:
            return f"Error: Tool '{tool_name}' not found"
        
        try:
            handler = self.tool_handlers[tool_name]
            result = handler(**tool_params)
            return result
        except Exception as e:
            return f"Error executing tool {tool_name}: {str(e)}"

    def stream_assistant_response(self, messages: List[Dict[str, Any]]) -> Any:
        """
        Stream the assistant's response with tool support.

        Args:
            messages (List[Dict[str, Any]]): The conversation messages

        Returns:
            A streaming response generator
        """
        # Convert messages to Google GenAI format
        google_messages = self._convert_messages_to_google_format(messages)
        
        # Create configuration with tools
        config = types.GenerateContentConfig(
            temperature=0.6,
            top_p=0.95,
            max_output_tokens=8192,
        )
        
        # Add system instruction if available
        if self.system_prompt:
            config.system_instruction = self.system_prompt
        
        # Add tools if available
        if self.tools:
            config.tools = self.tools
        
        # Return streaming generator
        return self.client.models.generate_content_stream(
            model=self.model,
            contents=google_messages,
            config=config
        )

    def _convert_messages_to_google_format(self, messages: List[Dict[str, Any]]):
        """
        Convert standard messages to Google GenAI format.

        Args:
            messages (List[Dict[str, Any]]): Standard message format

        Returns:
            List or str: Messages in Google GenAI format
        """
        google_messages = []
        
        for msg in messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            
            if role == "user":
                # User messages are simple strings in Google format
                google_messages.append(content)
            elif role == "assistant":
                # Assistant messages become model responses
                google_msg = {
                    "role": "model",
                    "parts": [{"text": content}]
                }
                
                # Add tool calls if present
                if "tool_calls" in msg:
                    for tool_call in msg["tool_calls"]:
                        function_call = {
                            "name": tool_call.get("name", ""),
                            "args": tool_call.get("arguments", {})
                        }
                        
                        # If arguments is a string (JSON), parse it
                        if isinstance(function_call["args"], str):
                            try:
                                function_call["args"] = json.loads(function_call["args"])
                            except:
                                pass
                                
                        google_msg["parts"].append({"function_call": function_call})
                
                google_messages.append(google_msg)
            elif role == "tool":
                # Tool responses are treated as user messages
                # This is because Google's API expects tool responses as part of the user's next message
                google_messages.append(f"Tool Response: {content}")
            elif role == "system":
                # System messages are handled via system_instruction in config
                pass
        
        return google_messages

    def process_stream_chunk(
        self, chunk, assistant_response: str, tool_uses: List[Dict]
    ) -> Tuple[str, List[Dict], int, int, Optional[str], Optional[tuple]]:
        """
        Process a single chunk from the streaming response.

        Args:
            chunk: The chunk from the stream
            assistant_response: Current accumulated assistant response
            tool_uses: Current tool use information

        Returns:
            tuple: (
                updated_assistant_response,
                updated_tool_uses,
                input_tokens,
                output_tokens,
                chunk_text,
                thinking_data
            )
        """
        chunk_text = None
        input_tokens = 0
        output_tokens = 0
        thinking_data = None
        
        # Handle text chunks
        if hasattr(chunk, "text") and chunk.text:
            chunk_text = chunk.text
            assistant_response += chunk_text
        
        # Handle function calls
        if hasattr(chunk, "candidates") and chunk.candidates:
            for candidate in chunk.candidates:
                if hasattr(candidate, "content") and candidate.content:
                    for part in candidate.content.parts:
                        if hasattr(part, "function_call") and part.function_call:
                            # Create a unique ID for the tool call
                            tool_id = f"{part.function_call.name}_{len(tool_uses)}"
                            
                            # Add to tool uses
                            tool_uses.append({
                                "id": tool_id,
                                "name": part.function_call.name,
                                "input": part.function_call.args,
                                "type": "function",
                                "response": ""
                            })
        
        # Handle usage information
        if hasattr(chunk, "usage"):
            if hasattr(chunk.usage, "prompt_tokens"):
                input_tokens = chunk.usage.prompt_tokens
            if hasattr(chunk.usage, "completion_tokens"):
                output_tokens = chunk.usage.completion_tokens
        
        return (
            assistant_response or " ",
            tool_uses,
            input_tokens,
            output_tokens,
            chunk_text,
            thinking_data
        )

    def format_tool_result(
        self, tool_use: Dict, tool_result: Any, is_error: bool = False
    ) -> Dict[str, Any]:
        """
        Format a tool result for the Google GenAI API.

        Args:
            tool_use: The tool use details
            tool_result: The result from the tool execution
            is_error: Whether the result is an error

        Returns:
            A formatted message for tool response
        """
        content = str(tool_result)
        if is_error:
            content = f"ERROR: {content}"
        
        # Return in a format expected by the interactive chat
        return {
            "role": "tool",
            "tool_call_id": tool_use["id"],
            "content": content
        }

    def format_assistant_message(
        self, assistant_response: str, tool_uses: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        Format the assistant's response for the Google GenAI API.

        Args:
            assistant_response: The response text
            tool_uses: List of tool use details

        Returns:
            Formatted assistant message
        """
        if tool_uses and any(tu.get("id") for tu in tool_uses):
            # Assistant message with tool calls
            return {
                "role": "assistant",
                "content": assistant_response,
                "tool_calls": [
                    {
                        "id": tool_use["id"],
                        "name": tool_use["name"],
                        "arguments": tool_use["input"],
                        "type": tool_use["type"]
                    }
                    for tool_use in tool_uses
                    if tool_use.get("id")  # Only include tool calls with valid IDs
                ]
            }
        else:
            # Simple assistant message
            return {
                "role": "assistant",
                "content": assistant_response
            }

    def format_thinking_message(self, thinking_data) -> Optional[Dict[str, Any]]:
        """
        Format thinking content into the appropriate message format.
        Not supported by Google GenAI.

        Args:
            thinking_data: Tuple containing (thinking_content, thinking_signature)
                or None if no thinking data is available

        Returns:
            Dict[str, Any]: A properly formatted message containing thinking blocks
        """
        # Google doesn't support thinking blocks
        return None

    def validate_spec(self, prompt: str) -> str:
        """
        Validate a specification prompt using Google GenAI.

        Args:
            prompt: The specification prompt to validate

        Returns:
            Validation result as a JSON string
        """
        try:
            # Request JSON response
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
            
            # Calculate and log token usage
            input_tokens = 0
            output_tokens = 0
            if hasattr(response, "usage") and response.usage:
                if hasattr(response.usage, "prompt_tokens"):
                    input_tokens = response.usage.prompt_tokens
                if hasattr(response.usage, "completion_tokens"):
                    output_tokens = response.usage.completion_tokens
            
            # Calculate cost
            total_cost = self.calculate_cost(input_tokens, output_tokens)
            
            print("\nSpec Validation Token Usage:")
            print(f"Input tokens: {input_tokens:,}")
            print(f"Output tokens: {output_tokens:,}")
            print(f"Total tokens: {input_tokens + output_tokens:,}")
            print(f"Estimated cost: ${total_cost:.4f}")
            
            # Return the response text (should be JSON)
            return response.text
        except Exception as e:
            raise Exception(f"Failed to validate specification: {str(e)}")

    def set_system_prompt(self, system_prompt: str):
        """
        Set the system prompt for the Google GenAI service.

        Args:
            system_prompt: The system prompt to use
        """
        self.system_prompt = system_prompt

    def clear_tools(self):
        """
        Clear all registered tools from the Google GenAI service.
        """
        self.tools = []
        self.tool_handlers = {}
        self.tool_definitions = []
```

Now, let me also provide the necessary updates to the main.py file to properly initialize and use this new service:

```python
# Update in main.py

def setup_services(provider):
    # Initialize the model registry and service manager
    registry = ModelRegistry.get_instance()
    manager = ServiceManager.get_instance()

    # Set the current model based on provider
    models = registry.get_models_by_provider(provider)
    if models:
        # Find default model for this provider
        default_model = next((m for m in models if m.default), models[0])
        registry.set_current_model(default_model.id)

    # Get the LLM service based on provider
    llm_service = None
    if provider == "google_native":
        try:
            from swissknife.modules.google.native_service import GoogleAINativeService
            llm_service = GoogleAINativeService()
        except Exception as e:
            click.echo(f"âŒ Error initializing Google AI native service: {str(e)}", err=True)
            # Fallback to OpenAI-compatible Google service
            provider = "google"
    
    if not llm_service:
        llm_service = manager.get_service(provider)

    # Initialize other services
    # ... (rest of the function)
```

And update the CLI options to include the native Google implementation:

```python
@cli.command()
@click.option("--message", help="Initial message to start the chat")
@click.option("--files", multiple=True, help="Files to include in the initial message")
@click.option(
    "--provider",
    type=click.Choice(["claude", "groq", "openai", "google", "google_native"]),
    default="claude",
    help="LLM provider to use (claude, groq, openai, google, or google_native)",
)
@click.option(
    "--agent",
    type=str,
    default="Architect",
    help="Initial agent to use (Architect, CodeAssistant, Documentation)",
)
def chat(message, files, provider, agent):
    """Start an interactive chat session with LLM"""
    try:
        services = setup_services(provider)
        
        # Set up the agent system
        agent_manager = setup_agents(services)
        
        # Select the initial agent if specified
        if agent:
            if not agent_manager.select_agent(agent):
                available_agents = ", ".join(agent_manager.agents.keys())
                click.echo(
                    f"âš ï¸ Unknown agent: {agent}. Using default agent. Available agents: {available_agents}"
                )
        
        # Create the chat interface with the agent manager injected
        chat_interface = InteractiveChat(services["memory"])
        
        # Start the chat
        chat_interface.start_chat(initial_content=message, files=files)
    except Exception as e:
        click.echo(f"âŒ Error: {str(e)}", err=True)
```

You'll also need to update the message transformer to properly handle the Google native format:

```python
# Add to swissknife/modules/llm/message.py

@staticmethod
def standardize_google_native_messages(messages):
    """Convert Google GenAI native messages to standard format."""
    standardized = []
    
    for i, msg in enumerate(messages):
        # Handle different message formats
        if isinstance(msg, str):
            # Simple user message
            standardized.append({
                "role": "user",
                "content": msg
            })
        elif isinstance(msg, dict):
            if msg.get("role") == "model":
                # Convert model message to assistant
                text_content = ""
                tool_calls = []
                
                for part in msg.get("parts", []):
                    if isinstance(part, str):
                        text_content += part
                    elif isinstance(part, dict):
                        if "text" in part:
                            text_content += part["text"]
                        elif "function_call" in part:
                            function_call = part["function_call"]
                            tool_calls.append({
                                "id": f"{function_call.get('name', '')}_id_{i}",
                                "name": function_call.get("name", ""),
                                "arguments": function_call.get("args", {}),
                                "type": "function"
                            })
                
                # Create standard assistant message
                std_msg = {
                    "role": "assistant",
                    "content": text_content
                }
                
                if tool_calls:
                    std_msg["tool_calls"] = tool_calls
                
                standardized.append(std_msg)
    
    return standardized

@staticmethod
def convert_to_google_native_format(messages):
    """Convert standard messages to Google GenAI native format."""
    google_messages = []
    
    for msg in messages:
        role = msg.get("role", "")
        
        if role == "user":
            # User messages are simple strings
            google_messages.append(msg.get("content", ""))
        elif role == "assistant":
            # Assistant messages are model messages with parts
            parts = []
            
            # Add text content
            if msg.get("content"):
                parts.append({"text": msg["content"]})
            
            # Add function calls
            for tool_call in msg.get("tool_calls", []):
                parts.append({
                    "function_call": {
                        "name": tool_call.get("name", ""),
                        "args": tool_call.get("arguments", {})
                    }
                })
            
            google_messages.append({
                "role": "model",
                "parts": parts
            })
        elif role == "tool":
            # Tool responses become user messages
            tool_response = f"Tool response: {msg.get('content', '')}"
            google_messages.append(tool_response)
    
    return google_messages
```

Finally, you should also update the service_manager.py to be aware of this new service:

```python
# Update in swissknife/modules/llm/service_manager.py

def get_service(self, provider: str):
    """Get the LLM service for the specified provider."""
    if provider == "google_native":
        # Special case for native Google service
        try:
            from swissknife.modules.google.native_service import GoogleAINativeService
            return GoogleAINativeService()
        except ImportError:
            print("âŒ Google GenAI native service not available")
            # Fall back to default provider
            provider = "claude"
    
    # Regular service loading
    # ... (rest of the method)
```

This full implementation provides a complete GoogleAINativeService that:

1. Uses the Google GenAI SDK directly instead of through OpenAI compatibility
2. Handles all BaseLLMService required methods
3. Provides proper message conversion between formats
4. Integrates with the existing tool system
5. Supports streaming responses and function calling

The implementation also includes the necessary updates to the main application to support using this new service alongside the existing ones.
Now I understand how the InteractiveChat class uses a context manager with the streaming response from LLM services. Let's review the key parts of the code related to streaming:

1. The key issue is in `_stream_assistant_response` method:

```python
with self.llm.stream_assistant_response(messages) as stream:
    for chunk in stream:
        # Process the chunk...
```

2. This code expects `stream_assistant_response` to return an object that supports the context manager protocol (with `__enter__` and `__exit__` methods) and is also iterable.

Let's now revise the GoogleAINativeService implementation to properly support the context manager protocol for streaming responses:

```python
# swissknife/modules/google/native_service.py
import os
import json
import base64
import mimetypes
from typing import Dict, Any, List, Optional, Tuple
from dotenv import load_dotenv
from google import genai
from google.genai import types
from contextlib import contextmanager
from swissknife.modules.llm.base import BaseLLMService
from swissknife.modules.llm.message import MessageTransformer
from swissknife.modules.prompts.constants import (
    EXPLAIN_PROMPT,
    SUMMARIZE_PROMPT,
    CHAT_SYSTEM_PROMPT,
)


class GoogleStreamAdapter:
    """
    Adapter class that wraps Google GenAI streaming response to support context manager protocol.
    """
    
    def __init__(self, stream_generator):
        """
        Initialize the adapter with a Google GenAI stream generator.
        
        Args:
            stream_generator: The generator returned by generate_content_stream
        """
        self.stream_generator = stream_generator
        
    def __enter__(self):
        """Enter the context manager, returning self as the iterable."""
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit the context manager, handling cleanup if needed."""
        # No specific cleanup needed for Google GenAI stream
        pass
        
    def __iter__(self):
        """Return an iterator for the stream."""
        return self
        
    def __next__(self):
        """Get the next chunk from the stream generator."""
        try:
            return next(self.stream_generator)
        except StopIteration:
            raise
        except Exception as e:
            # Handle any Google GenAI specific exceptions 
            print(f"Error in Google GenAI stream: {str(e)}")
            raise StopIteration


class GoogleAINativeService(BaseLLMService):
    """
    Google GenAI service implementation using the native Python SDK.
    This service connects to Google's Gemini models using the official Google GenAI SDK.
    """

    def __init__(self):
        """Initialize the Google GenAI service."""
        load_dotenv()
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found in environment variables")
        
        # Initialize the Google GenAI client
        self.client = genai.Client(api_key=api_key)
        
        # Default model
        self.model = "gemini-2.0-flash"
        
        # Initialize tools and handlers
        self.tools = []
        self.tool_handlers = {}
        self.tool_definitions = []  # Keep original definitions for reference
        
        # Provider name and system prompt
        self._provider_name = "google_native"
        self.system_prompt = CHAT_SYSTEM_PROMPT

    def set_think(self, budget_tokens: int) -> bool:
        """
        Enable or disable thinking mode with the specified token budget.
        Currently not supported in Google GenAI.

        Args:
            budget_tokens (int): Token budget for thinking. 0 to disable thinking mode.

        Returns:
            bool: True if thinking mode is supported and successfully set, False otherwise.
        """
        print("Thinking mode is not supported for Google GenAI models.")
        return False

    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """
        Calculate the cost based on token usage.

        Args:
            input_tokens (int): Number of input tokens
            output_tokens (int): Number of output tokens

        Returns:
            float: Estimated cost in USD
        """
        # Current Gemini pricing as of March 2025
        input_rate = 0.00000025  # $0.00025 per 1000 input tokens
        output_rate = 0.0000005  # $0.0005 per 1000 output tokens
        
        cost = (input_tokens * input_rate) + (output_tokens * output_rate)
        return cost

    def _process_content(self, prompt_template, content, max_tokens=2048):
        """
        Process content with a given prompt template.

        Args:
            prompt_template (str): The template with {content} placeholder
            content (str): The content to process
            max_tokens (int): Maximum tokens for the response

        Returns:
            str: The processed content
        """
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt_template.format(content=content),
                config=types.GenerateContentConfig(
                    max_output_tokens=max_tokens,
                    temperature=0.2
                )
            )

            # Get token usage if available
            input_tokens = 0
            output_tokens = 0
            if hasattr(response, "usage") and response.usage:
                if hasattr(response.usage, "prompt_tokens"):
                    input_tokens = response.usage.prompt_tokens
                if hasattr(response.usage, "completion_tokens"):
                    output_tokens = response.usage.completion_tokens

            # Calculate and log cost
            total_cost = self.calculate_cost(input_tokens, output_tokens)
            print("\nToken Usage Statistics:")
            print(f"Input tokens: {input_tokens:,}")
            print(f"Output tokens: {output_tokens:,}")
            print(f"Total tokens: {input_tokens + output_tokens:,}")
            print(f"Estimated cost: ${total_cost:.4f}")

            return response.text
        except Exception as e:
            raise Exception(f"Failed to process content: {str(e)}")

    def summarize_content(self, content: str) -> str:
        """
        Summarize the provided content using Google GenAI.

        Args:
            content (str): The content to summarize

        Returns:
            str: The summarized content
        """
        return self._process_content(SUMMARIZE_PROMPT, content, max_tokens=2048)

    def explain_content(self, content: str) -> str:
        """
        Explain the provided content using Google GenAI.

        Args:
            content (str): The content to explain

        Returns:
            str: The explained content
        """
        return self._process_content(EXPLAIN_PROMPT, content, max_tokens=1500)

    def process_file_for_message(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        Process a file and return the appropriate message content.

        Args:
            file_path (str): Path to the file

        Returns:
            Optional[Dict[str, Any]]: The message content or None if processing failed
        """
        try:
            mime_type, _ = mimetypes.guess_type(file_path)
            if not mime_type:
                mime_type = "application/octet-stream"
            
            # For text files, read directly
            if mime_type.startswith("text/"):
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                print(f"ðŸ“„ Including text file: {file_path}")
                return {
                    "type": "text",
                    "text": f"Content of {file_path}:\n\n{content}",
                }
            
            # For binary files, encode as base64
            with open(file_path, "rb") as f:
                content = f.read()
            encoded = base64.b64encode(content).decode("utf-8")
            
            print(f"ðŸ“„ Including file: {file_path} (MIME type: {mime_type})")
            return {
                "mime_type": mime_type,
                "data": encoded,
                "file_name": os.path.basename(file_path)
            }
        except Exception as e:
            print(f"âŒ Error processing file {file_path}: {str(e)}")
            return None

    def handle_file_command(self, file_path: str) -> Optional[List[Dict[str, Any]]]:
        """
        Handle the /file command and return message content.

        Args:
            file_path (str): Path to the file

        Returns:
            Optional[List[Dict[str, Any]]]: Message content or None if processing failed
        """
        result = self.process_file_for_message(file_path)
        if result:
            if "type" in result and result["type"] == "text":
                return [
                    {
                        "type": "text",
                        "text": f"I'm sharing this file with you:\n\n{result['text']}"
                    }
                ]
            else:
                # Upload the file using the Google GenAI SDK's file API
                try:
                    uploaded_file = self.client.files.upload(path=file_path)
                    return [
                        {
                            "type": "text",
                            "text": f"I'm sharing this file with you: {os.path.basename(file_path)}"
                        }
                    ]
                except Exception as e:
                    print(f"âŒ Error uploading file to Google GenAI: {str(e)}")
                    return None
        return None

    def register_tool(self, tool_definition, handler_function):
        """
        Register a tool with its handler function.

        Args:
            tool_definition (dict): The tool definition following OpenAI's function schema
            handler_function (callable): Function to call when tool is used
        """
        # Store original tool definition for reference
        self.tool_definitions.append(tool_definition)
        
        # Extract tool name from definition
        tool_name = self._extract_tool_name(tool_definition)
        
        # Extract parameters schema
        parameters = {}
        required = []
        
        if "function" in tool_definition:
            parameters = tool_definition["function"].get("parameters", {}).get("properties", {})
            required = tool_definition["function"].get("parameters", {}).get("required", [])
            description = tool_definition["function"].get("description", "")
        else:
            parameters = tool_definition.get("parameters", {}).get("properties", {})
            required = tool_definition.get("parameters", {}).get("required", [])
            description = tool_definition.get("description", "")
        
        # Create a function declaration for Google GenAI
        function_declaration = {
            "name": tool_name,
            "description": description,
            "parameters": {
                "type": "OBJECT",
                "properties": {}
            }
        }
        
        # Convert parameters to Google GenAI format
        for param_name, param_def in parameters.items():
            param_type = param_def.get("type", "STRING").upper()
            if param_type == "INTEGER":
                param_type = "NUMBER"
            
            function_declaration["parameters"]["properties"][param_name] = {
                "type": param_type,
                "description": param_def.get("description", "")
            }
        
        # Add required parameters
        if required:
            function_declaration["parameters"]["required"] = required
        
        # Create a Tool object with the function declaration
        self.tools.append(
            types.Tool(function_declarations=[function_declaration])
        )
        
        # Store the handler function
        self.tool_handlers[tool_name] = handler_function
        
        print(f"ðŸ”§ Registered tool: {tool_name}")

    def execute_tool(self, tool_name, tool_params) -> Any:
        """
        Execute a registered tool with the given parameters.

        Args:
            tool_name (str): Name of the tool to execute
            tool_params (dict): Parameters to pass to the tool

        Returns:
            Any: Result of the tool execution
        """
        if tool_name not in self.tool_handlers:
            return f"Error: Tool '{tool_name}' not found"
        
        try:
            handler = self.tool_handlers[tool_name]
            result = handler(**tool_params)
            return result
        except Exception as e:
            return f"Error executing tool {tool_name}: {str(e)}"

    def stream_assistant_response(self, messages: List[Dict[str, Any]]) -> Any:
        """
        Stream the assistant's response with tool support.
        Returns a context manager compatible adapter around the Google GenAI stream.

        Args:
            messages (List[Dict[str, Any]]): The conversation messages

        Returns:
            GoogleStreamAdapter: A context manager compatible adapter
        """
        # Convert messages to Google GenAI format
        google_messages = self._convert_messages_to_google_format(messages)
        
        # Create configuration with tools
        config = types.GenerateContentConfig(
            temperature=0.6,
            top_p=0.95,
            max_output_tokens=8192,
        )
        
        # Add system instruction if available
        if self.system_prompt:
            config.system_instruction = self.system_prompt
        
        # Add tools if available
        if self.tools:
            config.tools = self.tools
        
        # Get the stream generator
        stream_generator = self.client.models.generate_content_stream(
            model=self.model,
            contents=google_messages,
            config=config
        )
        
        # Wrap in adapter that supports context manager protocol
        return GoogleStreamAdapter(stream_generator)

    def _convert_messages_to_google_format(self, messages: List[Dict[str, Any]]):
        """
        Convert standard messages to Google GenAI format.

        Args:
            messages (List[Dict[str, Any]]): Standard message format

        Returns:
            List or str: Messages in Google GenAI format
        """
        google_messages = []
        
        for msg in messages:
            role = msg.get("role", "")
            
            # Handle different content formats
            content = msg.get("content", "")
            
            # If content is a list (Claude format), extract text parts
            if isinstance(content, list):
                text_parts = []
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_parts.append(item.get("text", ""))
                if text_parts:
                    content = "\n".join(text_parts)
            
            if role == "user":
                # User messages are simple strings in Google format
                google_messages.append(content)
            elif role == "assistant":
                # Assistant messages become model responses
                google_msg = {
                    "role": "model",
                    "parts": [{"text": content}]
                }
                
                # Add tool calls if present
                if "tool_calls" in msg:
                    for tool_call in msg["tool_calls"]:
                        function_call = {
                            "name": tool_call.get("name", ""),
                            "args": tool_call.get("arguments", {})
                        }
                        
                        # If arguments is a string (JSON), parse it
                        if isinstance(function_call["args"], str):
                            try:
                                function_call["args"] = json.loads(function_call["args"])
                            except:
                                pass
                                
                        google_msg["parts"].append({"function_call": function_call})
                
                google_messages.append(google_msg)
            elif role == "tool":
                # Tool responses are treated as user messages
                # This is because Google's API expects tool responses as part of the user's next message
                google_messages.append(f"Tool Response: {content}")
            elif role == "system":
                # System messages are handled via system_instruction in config
                pass
        
        return google_messages

    def process_stream_chunk(
        self, chunk, assistant_response: str, tool_uses: List[Dict]
    ) -> Tuple[str, List[Dict], int, int, Optional[str], Optional[tuple]]:
        """
        Process a single chunk from the streaming response.

        Args:
            chunk: The chunk from the stream
            assistant_response: Current accumulated assistant response
            tool_uses: Current tool use information

        Returns:
            tuple: (
                updated_assistant_response,
                updated_tool_uses,
                input_tokens,
                output_tokens,
                chunk_text,
                thinking_data
            )
        """
        chunk_text = None
        input_tokens = 0
        output_tokens = 0
        thinking_data = None
        
        # Process text content
        if hasattr(chunk, "text") and chunk.text:
            chunk_text = chunk.text
            assistant_response += chunk_text
        
        # Process candidates with function calls
        if hasattr(chunk, "candidates") and chunk.candidates:
            for candidate in chunk.candidates:
                if hasattr(candidate, "content") and candidate.content:
                    for part in candidate.content.parts:
                        if hasattr(part, "function_call") and part.function_call:
                            function_call = part.function_call
                            
                            # Create a unique ID for this tool call
                            tool_id = f"{function_call.name}_{len(tool_uses)}"
                            
                            # Check if this function call already exists in tool_uses
                            existing_tool = next(
                                (t for t in tool_uses if t.get("name") == function_call.name),
                                None
                            )
                            
                            if existing_tool:
                                # Update existing tool call if needed
                                if "args" in function_call and function_call.args:
                                    existing_tool["input"] = function_call.args
                            else:
                                # Add new tool call
                                tool_call = {
                                    "id": tool_id,
                                    "name": function_call.name,
                                    "input": function_call.args if hasattr(function_call, "args") else {},
                                    "type": "function",
                                    "response": ""
                                }
                                tool_uses.append(tool_call)
        
        # Process usage information
        if hasattr(chunk, "usage"):
            if hasattr(chunk.usage, "prompt_tokens"):
                input_tokens = chunk.usage.prompt_tokens
            if hasattr(chunk.usage, "completion_tokens"):
                output_tokens = chunk.usage.completion_tokens
        
        return (
            assistant_response or " ",
            tool_uses,
            input_tokens,
            output_tokens,
            chunk_text,
            thinking_data
        )

    def format_tool_result(
        self, tool_use: Dict, tool_result: Any, is_error: bool = False
    ) -> Dict[str, Any]:
        """
        Format a tool result for the Google GenAI API.

        Args:
            tool_use: The tool use details
            tool_result: The result from the tool execution
            is_error: Whether the result is an error

        Returns:
            A formatted message for tool response
        """
        content = str(tool_result)
        if is_error:
            content = f"ERROR: {content}"
        
        # Return in a format expected by the interactive chat
        return {
            "role": "tool",
            "tool_call_id": tool_use["id"],
            "content": content
        }

    def format_assistant_message(
        self, assistant_response: str, tool_uses: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        Format the assistant's response for the Google GenAI API.

        Args:
            assistant_response: The response text
            tool_uses: List of tool use details

        Returns:
            Formatted assistant message
        """
        if tool_uses and any(tu.get("id") for tu in tool_uses):
            # Assistant message with tool calls
            return {
                "role": "assistant",
                "content": assistant_response,
                "tool_calls": [
                    {
                        "id": tool_use["id"],
                        "name": tool_use["name"],
                        "arguments": tool_use["input"],
                        "type": tool_use["type"]
                    }
                    for tool_use in tool_uses
                    if tool_use.get("id")  # Only include tool calls with valid IDs
                ]
            }
        else:
            # Simple assistant message
            return {
                "role": "assistant",
                "content": assistant_response
            }

    def format_thinking_message(self, thinking_data) -> Optional[Dict[str, Any]]:
        """
        Format thinking content into the appropriate message format.
        Not supported by Google GenAI.

        Args:
            thinking_data: Tuple containing (thinking_content, thinking_signature)
                or None if no thinking data is available

        Returns:
            Dict[str, Any]: A properly formatted message containing thinking blocks
        """
        # Google doesn't support thinking blocks
        return None

    def validate_spec(self, prompt: str) -> str:
        """
        Validate a specification prompt using Google GenAI.

        Args:
            prompt: The specification prompt to validate

        Returns:
            Validation result as a JSON string
        """
        try:
            # Request JSON response
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
            
            # Calculate and log token usage
            input_tokens = 0
            output_tokens = 0
            if hasattr(response, "usage") and response.usage:
                if hasattr(response.usage, "prompt_tokens"):
                    input_tokens = response.usage.prompt_tokens
                if hasattr(response.usage, "completion_tokens"):
                    output_tokens = response.usage.completion_tokens
            
            # Calculate cost
            total_cost = self.calculate_cost(input_tokens, output_tokens)
            
            print("\nSpec Validation Token Usage:")
            print(f"Input tokens: {input_tokens:,}")
            print(f"Output tokens: {output_tokens:,}")
            print(f"Total tokens: {input_tokens + output_tokens:,}")
            print(f"Estimated cost: ${total_cost:.4f}")
            
            # Return the response text (should be JSON)
            return response.text
        except Exception as e:
            raise Exception(f"Failed to validate specification: {str(e)}")

    def set_system_prompt(self, system_prompt: str):
        """
        Set the system prompt for the Google GenAI service.

        Args:
            system_prompt: The system prompt to use
        """
        self.system_prompt = system_prompt

    def clear_tools(self):
        """
        Clear all registered tools from the Google GenAI service.
        """
        self.tools = []
        self.tool_handlers = {}
        self.tool_definitions = []
```

The key changes I've made are:

1. **Added a GoogleStreamAdapter class** that wraps the stream generator returned by Google's SDK and makes it compatible with the context manager protocol. This adapter:
   - Implements `__enter__` and `__exit__` for the context manager protocol
   - Implements `__iter__` and `__next__` to make the object iterable
   - Properly handles error handling and iteration

2. **Updated stream_assistant_response** to return the adapter:

   ```python
   def stream_assistant_response(self, messages):
       # ... setup code ...
       stream_generator = self.client.models.generate_content_stream(...)
       return GoogleStreamAdapter(stream_generator)
   ```

3. **Improved message format conversion** to handle different message formats, especially:
   - Extracts text from list-type content (Claude format)
   - Converts properly between standard format and Google's format

4. **Enhanced process_stream_chunk** to properly extract text content and function calls from the Google GenAI response chunks

5. **Standardized tool handling** to ensure tool execution and result formatting work properly with the InteractiveChat class

This implementation ensures that the GoogleAINativeService can be used with the existing InteractiveChat class without modifications to the chat interface. The adapter pattern allows us to bridge the gap between Google's streaming API and the context manager protocol expected by the chat interface.
It looks like we're running into an issue with how the Google GenAI SDK expects message formats. The error is happening because we're trying to pass Claude-style messages (lists of dictionaries with 'type' and 'text' fields) directly to the Google GenAI SDK, which expects a different format.

Let's fix the message conversion in the `_convert_messages_to_google_format` method to properly handle the Claude-style message format:

```python
def _convert_messages_to_google_format(self, messages: List[Dict[str, Any]]):
    """
    Convert standard messages to Google GenAI format.

    Args:
        messages (List[Dict[str, Any]]): Standard message format

    Returns:
        List or str: Messages in Google GenAI format
    """
    google_messages = []
    
    for msg in messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        
        # Handle Claude-style message with list of content items
        if isinstance(content, list):
            # Extract text content from the list
            text_content = ""
            for item in content:
                if isinstance(item, dict):
                    if item.get("type") == "text":
                        text_content += item.get("text", "") + "\n"
            
            # Use the extracted text
            content = text_content.strip()
        
        if role == "user":
            # User messages must be simple strings or Part objects in Google format
            from google.genai.types import Part
            # Create a text part
            google_messages.append(Part.from_text(content))
        elif role == "assistant":
            # Assistant messages are model responses
            model_msg = {
                "role": "model",
                "parts": [{"text": content}]
            }
            
            # Add tool calls if present
            if "tool_calls" in msg:
                for tool_call in msg["tool_calls"]:
                    function_call = {
                        "name": tool_call.get("name", ""),
                        "args": tool_call.get("arguments", {})
                    }
                    
                    # If arguments is a string (JSON), parse it
                    if isinstance(function_call["args"], str):
                        try:
                            function_call["args"] = json.loads(function_call["args"])
                        except:
                            pass
                            
                    model_msg["parts"].append({"function_call": function_call})
            
            google_messages.append(model_msg)
        elif role == "tool":
            # Tool responses are handled as user messages
            google_messages.append(Part.from_text(f"Tool Response: {content}"))
        elif role == "system":
            # System messages are handled via system_instruction in config
            pass
    
    return google_messages
```

Let's update the entire implementation to fix this issue and ensure proper message handling:

```python
# swissknife/modules/google/native_service.py
import os
import json
import base64
import mimetypes
from typing import Dict, Any, List, Optional, Tuple
from dotenv import load_dotenv
from google import genai
from google.genai import types
from contextlib import contextmanager
from swissknife.modules.llm.base import BaseLLMService
from swissknife.modules.llm.message import MessageTransformer
from swissknife.modules.prompts.constants import (
    EXPLAIN_PROMPT,
    SUMMARIZE_PROMPT,
    CHAT_SYSTEM_PROMPT,
)


class GoogleStreamAdapter:
    """
    Adapter class that wraps Google GenAI streaming response to support context manager protocol.
    """
    
    def __init__(self, stream_generator):
        """
        Initialize the adapter with a Google GenAI stream generator.
        
        Args:
            stream_generator: The generator returned by generate_content_stream
        """
        self.stream_generator = stream_generator
        
    def __enter__(self):
        """Enter the context manager, returning self as the iterable."""
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit the context manager, handling cleanup if needed."""
        # No specific cleanup needed for Google GenAI stream
        pass
        
    def __iter__(self):
        """Return an iterator for the stream."""
        return self
        
    def __next__(self):
        """Get the next chunk from the stream generator."""
        try:
            return next(self.stream_generator)
        except StopIteration:
            raise
        except Exception as e:
            # Handle any Google GenAI specific exceptions 
            print(f"Error in Google GenAI stream: {str(e)}")
            raise StopIteration


class GoogleAINativeService(BaseLLMService):
    """
    Google GenAI service implementation using the native Python SDK.
    This service connects to Google's Gemini models using the official Google GenAI SDK.
    """

    def __init__(self):
        """Initialize the Google GenAI service."""
        load_dotenv()
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found in environment variables")
        
        # Initialize the Google GenAI client
        self.client = genai.Client(api_key=api_key)
        
        # Default model
        self.model = "gemini-2.0-flash"
        
        # Initialize tools and handlers
        self.tools = []
        self.tool_handlers = {}
        self.tool_definitions = []  # Keep original definitions for reference
        
        # Provider name and system prompt
        self._provider_name = "google_native"
        self.system_prompt = CHAT_SYSTEM_PROMPT

    def set_think(self, budget_tokens: int) -> bool:
        """
        Enable or disable thinking mode with the specified token budget.
        Currently not supported in Google GenAI.

        Args:
            budget_tokens (int): Token budget for thinking. 0 to disable thinking mode.

        Returns:
            bool: True if thinking mode is supported and successfully set, False otherwise.
        """
        print("Thinking mode is not supported for Google GenAI models.")
        return False

    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """
        Calculate the cost based on token usage.

        Args:
            input_tokens (int): Number of input tokens
            output_tokens (int): Number of output tokens

        Returns:
            float: Estimated cost in USD
        """
        # Current Gemini pricing as of March 2025
        input_rate = 0.00000025  # $0.00025 per 1000 input tokens
        output_rate = 0.0000005  # $0.0005 per 1000 output tokens
        
        cost = (input_tokens * input_rate) + (output_tokens * output_rate)
        return cost

    def _process_content(self, prompt_template, content, max_tokens=2048):
        """
        Process content with a given prompt template.

        Args:
            prompt_template (str): The template with {content} placeholder
            content (str): The content to process
            max_tokens (int): Maximum tokens for the response

        Returns:
            str: The processed content
        """
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt_template.format(content=content),
                config=types.GenerateContentConfig(
                    max_output_tokens=max_tokens,
                    temperature=0.2
                )
            )

            # Get token usage if available
            input_tokens = 0
            output_tokens = 0
            if hasattr(response, "usage") and response.usage:
                if hasattr(response.usage, "prompt_tokens"):
                    input_tokens = response.usage.prompt_tokens
                if hasattr(response.usage, "completion_tokens"):
                    output_tokens = response.usage.completion_tokens

            # Calculate and log cost
            total_cost = self.calculate_cost(input_tokens, output_tokens)
            print("\nToken Usage Statistics:")
            print(f"Input tokens: {input_tokens:,}")
            print(f"Output tokens: {output_tokens:,}")
            print(f"Total tokens: {input_tokens + output_tokens:,}")
            print(f"Estimated cost: ${total_cost:.4f}")

            return response.text
        except Exception as e:
            raise Exception(f"Failed to process content: {str(e)}")

    def summarize_content(self, content: str) -> str:
        """
        Summarize the provided content using Google GenAI.

        Args:
            content (str): The content to summarize

        Returns:
            str: The summarized content
        """
        return self._process_content(SUMMARIZE_PROMPT, content, max_tokens=2048)

    def explain_content(self, content: str) -> str:
        """
        Explain the provided content using Google GenAI.

        Args:
            content (str): The content to explain

        Returns:
            str: The explained content
        """
        return self._process_content(EXPLAIN_PROMPT, content, max_tokens=1500)

    def process_file_for_message(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        Process a file and return the appropriate message content.

        Args:
            file_path (str): Path to the file

        Returns:
            Optional[Dict[str, Any]]: The message content or None if processing failed
        """
        try:
            mime_type, _ = mimetypes.guess_type(file_path)
            if not mime_type:
                mime_type = "application/octet-stream"
            
            # For text files, read directly
            if mime_type.startswith("text/"):
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                print(f"ðŸ“„ Including text file: {file_path}")
                return {
                    "type": "text",
                    "text": f"Content of {file_path}:\n\n{content}",
                }
            
            # For binary files, encode as base64
            with open(file_path, "rb") as f:
                content = f.read()
            encoded = base64.b64encode(content).decode("utf-8")
            
            print(f"ðŸ“„ Including file: {file_path} (MIME type: {mime_type})")
            return {
                "mime_type": mime_type,
                "data": encoded,
                "file_name": os.path.basename(file_path)
            }
        except Exception as e:
            print(f"âŒ Error processing file {file_path}: {str(e)}")
            return None

    def handle_file_command(self, file_path: str) -> Optional[List[Dict[str, Any]]]:
        """
        Handle the /file command and return message content.

        Args:
            file_path (str): Path to the file

        Returns:
            Optional[List[Dict[str, Any]]]: Message content or None if processing failed
        """
        result = self.process_file_for_message(file_path)
        if result:
            if "type" in result and result["type"] == "text":
                return [
                    {
                        "type": "text",
                        "text": f"I'm sharing this file with you:\n\n{result['text']}"
                    }
                ]
            else:
                # For now, we'll just use text for file content
                return [
                    {
                        "type": "text",
                        "text": f"I'm sharing this file with you: {os.path.basename(file_path)}"
                    }
                ]
        return None

    def register_tool(self, tool_definition, handler_function):
        """
        Register a tool with its handler function.

        Args:
            tool_definition (dict): The tool definition following OpenAI's function schema
            handler_function (callable): Function to call when tool is used
        """
        # Store original tool definition for reference
        self.tool_definitions.append(tool_definition)
        
        # Extract tool name from definition
        tool_name = self._extract_tool_name(tool_definition)
        
        # Extract parameters schema
        parameters = {}
        required = []
        
        if "function" in tool_definition:
            parameters = tool_definition["function"].get("parameters", {}).get("properties", {})
            required = tool_definition["function"].get("parameters", {}).get("required", [])
            description = tool_definition["function"].get("description", "")
        else:
            parameters = tool_definition.get("parameters", {}).get("properties", {})
            required = tool_definition.get("parameters", {}).get("required", [])
            description = tool_definition.get("description", "")
        
        # Create a function declaration for Google GenAI
        function_declaration = {
            "name": tool_name,
            "description": description,
            "parameters": {
                "type": "OBJECT",
                "properties": {}
            }
        }
        
        # Convert parameters to Google GenAI format
        for param_name, param_def in parameters.items():
            param_type = param_def.get("type", "STRING").upper()
            if param_type == "INTEGER":
                param_type = "NUMBER"
            
            function_declaration["parameters"]["properties"][param_name] = {
                "type": param_type,
                "description": param_def.get("description", "")
            }
        
        # Add required parameters
        if required:
            function_declaration["parameters"]["required"] = required
        
        # Create a Tool object with the function declaration
        self.tools.append(
            types.Tool(function_declarations=[function_declaration])
        )
        
        # Store the handler function
        self.tool_handlers[tool_name] = handler_function
        
        print(f"ðŸ”§ Registered tool: {tool_name}")

    def execute_tool(self, tool_name, tool_params) -> Any:
        """
        Execute a registered tool with the given parameters.

        Args:
            tool_name (str): Name of the tool to execute
            tool_params (dict): Parameters to pass to the tool

        Returns:
            Any: Result of the tool execution
        """
        if tool_name not in self.tool_handlers:
            return f"Error: Tool '{tool_name}' not found"
        
        try:
            handler = self.tool_handlers[tool_name]
            result = handler(**tool_params)
            return result
        except Exception as e:
            return f"Error executing tool {tool_name}: {str(e)}"

    def stream_assistant_response(self, messages: List[Dict[str, Any]]) -> Any:
        """
        Stream the assistant's response with tool support.
        Returns a context manager compatible adapter around the Google GenAI stream.

        Args:
            messages (List[Dict[str, Any]]): The conversation messages

        Returns:
            GoogleStreamAdapter: A context manager compatible adapter
        """
        try:
            # Convert messages to Google GenAI format
            google_messages = self._convert_messages_to_google_format(messages)
            
            # Create configuration with tools
            config = types.GenerateContentConfig(
                temperature=0.6,
                top_p=0.95,
                max_output_tokens=8192,
            )
            
            # Add system instruction if available
            if self.system_prompt:
                config.system_instruction = self.system_prompt
            
            # Add tools if available
            if self.tools:
                config.tools = self.tools
            
            # Get the stream generator
            stream_generator = self.client.models.generate_content_stream(
                model=self.model,
                contents=google_messages,
                config=config
            )
            
            # Wrap in adapter that supports context manager protocol
            return GoogleStreamAdapter(stream_generator)
        except Exception as e:
            print(f"Error creating stream: {str(e)}")
            # Create a dummy adapter that returns an empty response
            class EmptyStreamAdapter:
                def __enter__(self):
                    return self
                def __exit__(self, exc_type, exc_val, exc_tb):
                    pass
                def __iter__(self):
                    return self
                def __next__(self):
                    raise StopIteration
            
            return EmptyStreamAdapter()

    def _convert_messages_to_google_format(self, messages: List[Dict[str, Any]]):
        """
        Convert standard messages to Google GenAI format.

        Args:
            messages (List[Dict[str, Any]]): Standard message format

        Returns:
            List: Messages in Google GenAI format as Content or Part objects
        """
        from google.genai.types import Content, Part
        
        # First, standardize messages to ensure consistent format
        standardized_messages = MessageTransformer.standardize_messages(messages, self._provider_name)
        
        # Create a conversation in Google format
        google_messages = []
        
        for msg in standardized_messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            
            if role == "user":
                # Create a user message
                google_messages.append(Content(
                    role="user",
                    parts=[Part.from_text(content)]
                ))
            
            elif role == "assistant":
                # Create an assistant message
                parts = [Part.from_text(content)]
                
                # Add tool calls if present
                if "tool_calls" in msg:
                    for tool_call in msg["tool_calls"]:
                        # We can't directly add function calls in this format
                        # so we'll add a text representation
                        tool_text = f"\nUsing tool: {tool_call.get('name', '')}\n"
                        tool_text += f"Arguments: {json.dumps(tool_call.get('arguments', {}), indent=2)}"
                        parts.append(Part.from_text(tool_text))
                
                google_messages.append(Content(
                    role="model",
                    parts=parts
                ))
            
            elif role == "tool":
                # Tool responses need to be sent as user messages
                tool_content = f"Tool result: {content}"
                google_messages.append(Content(
                    role="user", 
                    parts=[Part.from_text(tool_content)]
                ))
        
        return google_messages

    def process_stream_chunk(
        self, chunk, assistant_response: str, tool_uses: List[Dict]
    ) -> Tuple[str, List[Dict], int, int, Optional[str], Optional[tuple]]:
        """
        Process a single chunk from the streaming response.

        Args:
            chunk: The chunk from the stream
            assistant_response: Current accumulated assistant response
            tool_uses: Current tool use information

        Returns:
            tuple: (
                updated_assistant_response,
                updated_tool_uses,
                input_tokens,
                output_tokens,
                chunk_text,
                thinking_data
            )
        """
        chunk_text = None
        input_tokens = 0
        output_tokens = 0
        thinking_data = None
        
        # Process text content
        if hasattr(chunk, "text") and chunk.text:
            chunk_text = chunk.text
            assistant_response += chunk_text
        
        # Process function calls
        if hasattr(chunk, "candidates") and chunk.candidates:
            for candidate in chunk.candidates:
                if hasattr(candidate, "content") and candidate.content:
                    for part in candidate.content.parts:
                        # Check if this part has a function call
                        if hasattr(part, "function_call") and part.function_call:
                            function_call = part.function_call
                            
                            # Create a unique ID for this tool call
                            tool_id = f"{function_call.name}_{len(tool_uses)}"
                            
                            # Check if this function is already in tool_uses
                            existing_tool = next(
                                (t for t in tool_uses if t.get("name") == function_call.name), 
                                None
                            )
                            
                            if existing_tool:
                                # Update the existing tool
                                if hasattr(function_call, "args") and function_call.args:
                                    existing_tool["input"] = function_call.args
                            else:
                                # Create a new tool use entry
                                tool_uses.append({
                                    "id": tool_id,
                                    "name": function_call.name,
                                    "input": function_call.args if hasattr(function_call, "args") else {},
                                    "type": "function",
                                    "response": ""
                                })
        
        # Process usage information if available
        if hasattr(chunk, "usage"):
            if hasattr(chunk.usage, "prompt_tokens"):
                input_tokens = chunk.usage.prompt_tokens
            if hasattr(chunk.usage, "completion_tokens"):
                output_tokens = chunk.usage.completion_tokens
        
        return (
            assistant_response or " ",
            tool_uses,
            input_tokens,
            output_tokens,
            chunk_text,
            thinking_data
        )

    def format_tool_result(
        self, tool_use: Dict, tool_result: Any, is_error: bool = False
    ) -> Dict[str, Any]:
        """
        Format a tool result for the Google GenAI API.

        Args:
            tool_use: The tool use details
            tool_result: The result from the tool execution
            is_error: Whether the result is an error

        Returns:
            A formatted message for tool response
        """
        content = str(tool_result)
        if is_error:
            content = f"ERROR: {content}"
        
        # Return in a format expected by the interactive chat
        return {
            "role": "tool",
            "tool_call_id": tool_use["id"],
            "content": content
        }

    def format_assistant_message(
        self, assistant_response: str, tool_uses: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        Format the assistant's response for the Google GenAI API.

        Args:
            assistant_response: The response text
            tool_uses: List of tool use details

        Returns:
            Formatted assistant message
        """
        if tool_uses and any(tu.get("id") for tu in tool_uses):
            # Assistant message with tool calls
            return {
                "role": "assistant",
                "content": assistant_response,
                "tool_calls": [
                    {
                        "id": tool_use["id"],
                        "name": tool_use["name"],
                        "arguments": tool_use["input"],
                        "type": tool_use["type"]
                    }
                    for tool_use in tool_uses
                    if tool_use.get("id")  # Only include tool calls with valid IDs
                ]
            }
        else:
            # Simple assistant message
            return {
                "role": "assistant",
                "content": assistant_response
            }

    def format_thinking_message(self, thinking_data) -> Optional[Dict[str, Any]]:
        """
        Format thinking content into the appropriate message format.
        Not supported by Google GenAI.

        Args:
            thinking_data: Tuple containing (thinking_content, thinking_signature)
                or None if no thinking data is available

        Returns:
            Dict[str, Any]: A properly formatted message containing thinking blocks
        """
        # Google doesn't support thinking blocks
        return None

    def validate_spec(self, prompt: str) -> str:
        """
        Validate a specification prompt using Google GenAI.

        Args:
            prompt: The specification prompt to validate

        Returns:
            Validation result as a JSON string
        """
        try:
            # Request JSON response
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
            
            # Calculate and log token usage
            input_tokens = 0
            output_tokens = 0
            if hasattr(response, "usage") and response.usage:
                if hasattr(response.usage, "prompt_tokens"):
                    input_tokens = response.usage.prompt_tokens
                if hasattr(response.usage, "completion_tokens"):
                    output_tokens = response.usage.completion_tokens
            
            # Calculate cost
            total_cost = self.calculate_cost(input_tokens, output_tokens)
            
            print("\nSpec Validation Token Usage:")
            print(f"Input tokens: {input_tokens:,}")
            print(f"Output tokens: {output_tokens:,}")
            print(f"Total tokens: {input_tokens + output_tokens:,}")
            print(f"Estimated cost: ${total_cost:.4f}")
            
            # Return the response text (should be JSON)
            return response.text
        except Exception as e:
            raise Exception(f"Failed to validate specification: {str(e)}")

    def set_system_prompt(self, system_prompt: str):
        """
        Set the system prompt for the Google GenAI service.

        Args:
            system_prompt: The system prompt to use
        """
        self.system_prompt = system_prompt

    def clear_tools(self):
        """
        Clear all registered tools from the Google GenAI service.
        """
        self.tools = []
        self.tool_handlers = {}
        self.tool_definitions = []
```

The key changes I've made to address the error include:

1. **Using proper Google GenAI types**: Import and use `Content` and `Part` from `google.genai.types` to ensure message format correctness.

2. **Improved message conversion**:
   - Standardize messages first to ensure a consistent format
   - Create proper `Content` objects with the correct roles
   - Use `Part.from_text()` to create properly formatted text parts

3. **Better error handling**:
   - Added a try/except block in `stream_assistant_response`
   - Created a dummy adapter that returns an empty response in case of errors

4. **Simplified message processing**:
   - More direct mapping of standard roles to Google's role types
   - Better handling of Claude-style message formats

This implementation should now properly handle the message format conversion between the internal format used by the InteractiveChat class and the format expected by the Google GenAI SDK.

from abc import abstractmethod
from typing import List, Dict, Any, Tuple, Optional
import traceback
import os
import time

from swissknife.modules.agents.base import MessageType
from swissknife.modules.chat.history import ChatHistoryManager, ConversationTurn
from swissknife.modules.agents import AgentManager
from swissknife.modules.chat.file_handler import FileHandler
from swissknife.modules.llm.model_registry import ModelRegistry
from swissknife.modules.llm.service_manager import ServiceManager
from swissknife.modules.llm.message import MessageTransformer
from swissknife.modules.memory import MemoryService, ContextPersistenceService


class Observable:
    """Base class for observables, implementing the observer pattern."""

    def __init__(self):
        self._observers: List["Observer"] = []

    def attach(self, observer: "Observer"):
        """Attaches an observer to the observable."""
        if observer not in self._observers:
            self._observers.append(observer)

    def detach(self, observer: "Observer"):
        """Detaches an observer from the observable."""
        if observer in self._observers:
            self._observers.remove(observer)

    def _notify(self, event: str, data: Any = None):
        """Notifies all attached observers of a new event."""
        for observer in self._observers:
            observer.listen(event, data)


class Observer:
    """Abstract base class for observers."""

    @abstractmethod
    def listen(self, event: str, data: Any = None):
        """Updates the observer with new data from the observable."""
        pass


class MessageHandler(Observable):
    """
    Handles message processing, interaction with the LLM service, and manages
    conversation history. Uses the Observer pattern to notify UI components
    about relevant events.
    """

    def __init__(
        self,
        memory_service: MemoryService,
        context_persistent_service: ContextPersistenceService,
    ):
        """
        Initializes the MessageHandler.

        Args:
            llm_service: The LLM service to use for generating responses.
            memory_service: Optional memory service for storing conversations.
        """
        super().__init__()
        self.agent_manager = AgentManager.get_instance()
        self.agent = self.agent_manager.get_current_agent()
        # self.llm = self.agent_manager.get_current_agent().llm
        self.memory_service = memory_service
        self.persistent_service = context_persistent_service
        self.history_manager = ChatHistoryManager()
        self.latest_assistant_response = ""
        self.conversation_turns = []
        self.current_user_input = None
        self.current_user_input_idx = -1
        self.last_assisstant_response_idx = -1
        self.file_handler = FileHandler()
        # self.messages = []  # Initialize empty messages list
        self.streamline_messages = []
        self.current_conversation_id = None  # ID for persistence
        self.start_new_conversation()  # Initialize first conversation

    def _messages_append(self, message):
        self.agent.history.append(message)

        std_msg = MessageTransformer.standardize_messages(
            [message], self.agent.get_provider(), self.agent.name
        )
        self.streamline_messages.extend(std_msg)

    def process_user_input(
        self,
        user_input: str,
    ) -> Tuple[bool, bool]:
        """
        Processes user input, handles commands, and updates message history.

        Args:
            user_input: The input string from the user.
            message_content: Optional content to prepend.
            files: Optional files to include.

        Returns:
            Tuple of (exit_flag, clear_flag)
        """
        # Handle exit command
        if self._handle_exit_command(user_input):
            self._notify("exit_requested")
            return True, False

        # Handle clear command
        if user_input.lower() == "/clear":
            # Now handled by start_new_conversation
            self.start_new_conversation()
            return False, True  # exit_flag, clear_flag

        # Handle copy command
        if user_input.lower() == "/copy":
            self._notify("copy_requested", self.latest_assistant_response)
            return False, True  # Skip to next iteration

        # Handle debug command
        if user_input.lower() == "/debug":
            self._notify("debug_requested", self.agent.history)
            self._notify("debug_requested", self.streamline_messages)
            return False, True

        # Handle think command
        if user_input.lower().startswith("/think "):
            try:
                budget = user_input[7:].strip()
                self.agent.configure_think(budget)
                self._notify("think_budget_set", budget)
            except ValueError:
                self._notify("error", "Invalid budget value. Please provide a number.")
            return False, True

        # Handle jump command
        if user_input.lower().startswith("/jump "):
            jumped = self._handle_jump_command(user_input)
            if jumped:
                return False, True
            return False, True

        # Handle agent command
        if user_input.lower().startswith("/agent"):
            success, message = self._handle_agent_command(user_input)
            self._notify(
                "agent_command_result", {"success": success, "message": message}
            )
            return False, True

        # Handle model command
        if user_input.lower().startswith("/model"):
            return self._handle_model_command(user_input)

        # Store non-command messages in history
        if not user_input.startswith("/"):
            self.history_manager.add_entry(user_input)

        # if len(self.agent.history) == 0:
        #     self.agent.history.append(
        #         {
        #             "role": "user",
        #             "content": [
        #                 {
        #                     "type": "text",
        #                     "text": f"""Use user_context_summary to tailor your response:
        #                     <user_context_summary>{self.persistent_service.get_user_context_json(2, 3)}</user_context_summary>""",
        #                 }
        #             ],
        #         }
        #     )

        # Handle files that were loaded but not yet sent
        # Handle file command
        if user_input.startswith("/file "):
            file_path = user_input[6:].strip()
            file_path = os.path.expanduser(file_path)

            # Process file with the file handling service
            file_content = self.file_handler.process_file(file_path)
            # Fallback to llm handle
            if not file_content:
                file_content = self.agent.format_message(
                    MessageType.FileContent, {"file_uri": file_path}
                )

            if file_content:
                self._messages_append({"role": "user", "content": [file_content]})
                if file_content.get("type", "") == "text":
                    # TODO: For testing retrieve with keywords when transfer
                    self.memory_service.store_conversation(
                        file_content.get("text", ""), ""
                    )
                self._notify(
                    "file_processed",
                    {"file_path": file_path, "message": self.agent.history[-1]},
                )
                return False, True
            else:
                self._notify(
                    "error",
                    f"Failed to process file {file_path} Or Model is not supported",
                )
                return False, True
        else:
            # Add regular text message
            self._messages_append(
                {"role": "user", "content": [{"type": "text", "text": user_input}]}
            )
            self.current_user_input = self.agent.history[-1]
            self.current_user_input_idx = len(self.streamline_messages) - 1
            self._notify(
                "user_message_created",
                {"message": self.agent.history[-1], "with_files": False},
            )

        return False, False

    def start_new_conversation(self):
        """Starts a new persistent conversation, clears history, and gets a new ID."""
        try:
            # Ensure the service instance is available
            if (
                not hasattr(self, "persistent_service")
                or self.persistent_service is None
            ):
                raise RuntimeError(
                    "ContextPersistenceService not initialized in MessageHandler."
                )

            self.current_conversation_id = self.persistent_service.start_conversation()
            # self.messages = []  # Clear in-memory message list
            self.agent_manager.clean_agents_messages()
            self.streamline_messages = []
            self.conversation_turns = []  # Clear jump history
            self.last_assisstant_response_idx = 0
            self.current_user_input = None
            self.current_user_input_idx = -1
            # Notify UI about the new conversation
            self._notify(
                "system_message",
                f"Started new conversation: {self.current_conversation_id}",
            )
            # Re-use existing signal to clear UI display, ensures UI is reset
            self._notify("clear_requested")
            print(
                f"INFO: Started new persistent conversation {self.current_conversation_id}"
            )
        except Exception as e:
            error_message = f"Failed to start new persistent conversation: {str(e)}"
            print(f"ERROR: {error_message}")
            self._notify("error", {"message": error_message})

            self.current_conversation_id = (
                None  # Ensure saving fails safely if start fails
            )

    def _handle_exit_command(self, user_input: str) -> bool:
        """Check if the user wants to exit the chat."""
        return user_input.lower() in ["exit", "quit"]

    def _handle_jump_command(self, command: str) -> bool:
        """Handle the /jump command to rewind conversation to a previous turn."""
        try:
            # Extract the turn number from the command
            parts = command.split()
            if len(parts) != 2:
                self._notify("error", "Usage: /jump <turn_number>")
                return False

            turn_number = int(parts[1])

            # Validate the turn number
            if turn_number < 1 or turn_number > len(self.conversation_turns):
                self._notify(
                    "error",
                    f"Invalid turn number. Available turns: 1-{len(self.conversation_turns)}",
                )
                return False

            # Get the selected turn
            selected_turn = self.conversation_turns[turn_number - 1]

            # Provide feedback via notification

            # Truncate messages to the index from the selected turn
            self.streamline_messages = self.streamline_messages[
                : selected_turn.message_index
            ]
            if self.current_conversation_id:
                self.persistent_service.append_conversation_messages(
                    self.current_conversation_id,
                    self.streamline_messages,
                    True,
                )

            # Get the last assistant message from the streamline messages
            last_assistant_message = next(
                (
                    msg
                    for msg in reversed(self.streamline_messages)
                    if msg.get("role") == "assistant"
                ),
                None,
            )
            if last_assistant_message and last_assistant_message.get("agent", ""):
                self._handle_agent_command(f"/agent {last_assistant_message['agent']}")

            # TODO: Check this part when we already rebuild messages for agents
            # self.agent.history = MessageTransformer.convert_messages(
            #     [
            #         msg
            #         for msg in self.streamline_messages
            #         if msg["role"] != "assistant" or msg["agent"] == self.agent.name
            #     ],
            #     self.agent.llm.provider_name,
            # )
            self.agent_manager.rebuild_agents_messages(self.streamline_messages)
            self.conversation_turns = self.conversation_turns[: turn_number - 1]
            self.last_assisstant_response_idx = len(self.agent.history)

            self._notify(
                "jump_performed",
                {"turn_number": turn_number, "preview": selected_turn.get_preview(100)},
            )

            # Return success
            return True

        except ValueError:
            self._notify("error", "Invalid turn number. Please provide a number.")
            return False

    def _handle_model_command(self, command: str) -> Tuple[bool, bool]:
        """
        Handle the /model command to switch models or list available models.

        Args:
            command: The model command string

        Returns:
            Tuple of (exit_flag, clear_flag)
        """
        model_id = command[7:].strip()
        registry = ModelRegistry.get_instance()
        manager = ServiceManager.get_instance()

        # If no model ID is provided, list available models
        if not model_id:
            models_by_provider = {}
            for provider in ["claude", "openai", "groq", "google"]:
                models = registry.get_models_by_provider(provider)
                if models:
                    models_by_provider[provider] = []
                    for model in models:
                        current = (
                            registry.current_model
                            and registry.current_model.id == model.id
                        )
                        models_by_provider[provider].append(
                            {
                                "id": model.id,
                                "name": model.name,
                                "description": model.description,
                                "capabilities": model.capabilities,
                                "current": current,
                            }
                        )

            self._notify("models_listed", models_by_provider)
            return False, True

        # Try to switch to the specified model
        if registry.set_current_model(model_id):
            model = registry.get_current_model()
            if model:
                # Update the LLM service
                manager.set_model(model.provider, model.id)

                # Get the new LLM service
                new_llm_service = manager.get_service(model.provider)

                # Update the agent manager with the new LLM service
                self.agent_manager.update_llm_service(new_llm_service)

                # Update our reference to the LLM
                # self..llm = self.agent_manager.get_current_agent().llm

                self._notify(
                    "model_changed",
                    {"id": model.id, "name": model.name, "provider": model.provider},
                )
            else:
                self._notify("error", "Failed to switch model.")
        else:
            self._notify("error", f"Unknown model: {model_id}")

        return False, True

    def _handle_agent_command(self, command: str) -> Tuple[bool, str]:
        """
        Handle the /agent command to switch agents or list available agents.

        Args:
            command: The agent command string

        Returns:
            Tuple of (success, message)
        """
        parts = command.split()

        # If no agent name is provided, list available agents
        if len(parts) == 1:
            available_agents = list(self.agent_manager.agents.keys())

            agents_info = {"current": self.agent.name, "available": {}}

            for agent_name, agent in self.agent_manager.agents.items():
                agents_info["available"][agent_name] = {
                    "description": agent.description,
                    "current": (self.agent and self.agent.name == agent_name),
                }

            self._notify("agents_listed", agents_info)
            return True, "Listed available agents"

        # If an agent name is provided, try to switch to that agent
        agent_name = parts[1]
        if self.agent_manager.select_agent(agent_name):
            # Update the LLM reference to the new agent's LLM
            # self.llm = self.agent_manager.get_current_agent().llm
            self.agent = self.agent_manager.get_current_agent()
            self._notify("agent_changed", agent_name)
            return True, f"Switched to {agent_name} agent"
        else:
            available_agents = ", ".join(self.agent_manager.agents.keys())
            self._notify(
                "error",
                f"Unknown agent: {agent_name}. Available agents: {available_agents}",
            )
            return (
                False,
                f"Unknown agent: {agent_name}. Available agents: {available_agents}",
            )

    def _pre_tool_transfer(self):
        pass
        # transfered_agent = self.agent_manager.get_current_agent()
        # if transfered_agent:
        #     transfered_agent.history = MessageTransformer.standardize_messages(
        #         self.messages,
        #         self.agent.llm.provider_name,
        #         transfered_agent.name,
        #     )
        # return transfered_agent

    def _post_tool_transfer(self, tool_result, owner_agent):
        if (
            owner_agent
            and self.current_conversation_id
            and self.last_assisstant_response_idx >= 0
        ):
            self.persistent_service.append_conversation_messages(
                self.current_conversation_id,
                MessageTransformer.standardize_messages(
                    self.agent.history[self.last_assisstant_response_idx :],
                    self.agent.get_provider(),
                    owner_agent.name,
                ),
            )

        # Update llm service when transfer agent
        self.agent = self.agent_manager.get_current_agent()
        # self.llm = self.agent_manager.get_current_agent().llm

        # self.agent.history = MessageTransformer.convert_messages(
        #     self.agent_manager.get_current_agent().history,
        #     self.agent.llm.provider_name,
        # )

        self._messages_append(
            {
                "role": "user",
                "content": [{"type": "text", "text": tool_result}],
            }
        )
        if self.current_conversation_id:
            related_mesages_idx = tool_result.find("\n### Relevant messages")
            if related_mesages_idx >= 0:
                tool_result = tool_result[:related_mesages_idx]
            self.persistent_service.append_conversation_messages(
                self.current_conversation_id,
                [
                    {
                        "role": "user",
                        "agent": self.agent.name,
                        "content": [{"type": "text", "text": tool_result}],
                    }
                ],
            )
        self.last_assisstant_response_idx = len(self.agent.history)

        self._notify("agent_changed_by_transfer", self.agent.name)

    def get_assistant_response(
        self, input_tokens=0, output_tokens=0
    ) -> Tuple[Optional[str], int, int]:
        """
        Stream the assistant's response and return the response and token usage.

        Returns:
            Tuple of (assistant_response, input_tokens, output_tokens)
        """
        assistant_response = ""
        tool_uses = []
        thinking_content = ""  # Reset thinking content for new response
        thinking_signature = ""  # Store the signature
        start_thinking = False
        end_thinking = False
        try:
            for (
                assistant_response,
                chunk_text,
                thinking_chunk,
            ) in self.agent.process_messages():
                # Accumulate thinking content if available
                if thinking_chunk:
                    think_text_chunk, signature = thinking_chunk

                    if not start_thinking:
                        # Notify about thinking process
                        self._notify("thinking_started", self.agent.name)
                        if not self.agent.is_streaming():
                            # Delays it a bit when using without stream
                            time.sleep(0.5)
                        start_thinking = True
                    if think_text_chunk:
                        thinking_content += think_text_chunk
                        self._notify("thinking_chunk", think_text_chunk)
                    if signature:
                        thinking_signature += signature
                if chunk_text:
                    # End thinking when chunk_text start
                    if not end_thinking and start_thinking:
                        self._notify("thinking_completed")
                        end_thinking = True
                    # Notify about response progress
                    if not self.agent.is_streaming():
                        # Delays it a bit when using without stream
                        time.sleep(0.5)
                    self._notify("response_chunk", (chunk_text, assistant_response))

            tool_uses, input_tokens_in_turn, output_tokens_in_turn = (
                self.agent.get_process_result()
            )
            input_tokens += input_tokens_in_turn
            output_tokens_in_turn += output_tokens_in_turn
            # Handle tool use if needed
            if tool_uses and len(tool_uses) > 0:
                # Add thinking content as a separate message if available
                thinking_data = (
                    (thinking_content, thinking_signature) if thinking_content else None
                )
                thinking_message = self.agent.format_message(
                    MessageType.Thinking, {"thinking": thinking_data}
                )
                if thinking_message:
                    self._messages_append(thinking_message)
                    self._notify("thinking_message_added", thinking_message)

                # Format assistant message with the response and tool uses
                assistant_message = self.agent.format_message(
                    MessageType.Assistant,
                    {
                        "message": assistant_response,
                        "tool_uses": [t for t in tool_uses if t["name"] != "transfer"],
                    },
                )
                self._messages_append(assistant_message)
                self._notify("assistant_message_added", assistant_message)

                # Process each tool use
                for tool_use in tool_uses:
                    self._notify("response_completed", assistant_response)
                    self._notify("tool_use", tool_use)

                    try:
                        owner_agent = None
                        if tool_use["name"] == "transfer":
                            owner_agent = self._pre_tool_transfer()

                        tool_result = self.agent.execute_tool_call(
                            tool_use["name"], tool_use["input"]
                        )

                        if tool_use["name"] == "transfer":
                            self._post_tool_transfer(tool_result, owner_agent)

                        else:
                            tool_result_message = self.agent.format_message(
                                MessageType.ToolResult,
                                {"tool_use": tool_use, "tool_result": tool_result},
                            )
                            self._messages_append(tool_result_message)
                            self._notify(
                                "tool_result",
                                {
                                    "tool_use": tool_use,
                                    "tool_result": tool_result,
                                    "message": tool_result_message,
                                },
                            )

                        # if transfered_agent:
                        # transfered_agent.history.extend(
                        #     MessageTransformer.standardize_messages(
                        #         [tool_result_message],
                        #         self.agent.llm.provider_name,
                        #         transfered_agent.name,
                        #     )
                        # )

                    except Exception as e:
                        if tool_use["name"] == "transfer":
                            # if transfer failed we should add the tool_call message back for record
                            self._messages_append(
                                self.agent.format_message(
                                    MessageType.Assistant,
                                    {
                                        "message": assistant_response,
                                        "tool_uses": [tool_use],
                                    },
                                )
                            )

                        error_message = self.agent.format_message(
                            MessageType.ToolResult,
                            {
                                "tool_use": tool_use,
                                "tool_result": str(e),
                                "is_error": True,
                            },
                        )
                        self._messages_append(error_message)
                        self._notify(
                            "tool_error",
                            {
                                "tool_use": tool_use,
                                "error": str(e),
                                "message": error_message,
                            },
                        )

                return self.get_assistant_response(input_tokens, output_tokens)

            if thinking_content:
                # self._notify("thinking_completed")
                self._notify("agent_continue", self.agent.name)

            # Final assistant message
            self._notify("response_completed", assistant_response)

            # Store the latest response
            self.latest_assistant_response = assistant_response

            # Add assistant response to messages
            if assistant_response:
                self._messages_append(
                    self.agent.format_message(
                        MessageType.Assistant, {"message": assistant_response}
                    )
                )

            if self.current_user_input and self.current_user_input_idx >= 0:
                if self.memory_service:
                    user_input = ""
                    user_message = self.current_user_input  # Get the user message
                    if (
                        isinstance(user_message["content"], list)
                        and len(user_message["content"]) > 0
                    ):
                        for content_item in user_message["content"]:
                            if content_item.get("type") == "text":
                                user_input += content_item.get("text", "")
                    elif isinstance(user_message["content"], str):
                        user_input = user_message["content"]

                    try:
                        self.memory_service.store_conversation(
                            user_input, assistant_response
                        )
                    except Exception as e:
                        self._notify(
                            "error", f"Failed to store conversation in memory: {str(e)}"
                        )
                # Store the conversation turn reference for /jump command
                turn = ConversationTurn(
                    self.current_user_input,  # User message for preview
                    self.current_user_input_idx,  # Index of the *start* of this turn's messages
                )
                self.conversation_turns.append(turn)
                self.current_user_input = None
                self.current_user_input_idx = -1

            # --- Start of Persistence Logic ---
            if self.current_conversation_id and self.last_assisstant_response_idx >= 0:
                try:
                    # Get all messages added since the user input for this turn
                    current_provider = self.agent.get_provider()
                    messages_for_this_turn = MessageTransformer.standardize_messages(
                        self.agent.history[self.last_assisstant_response_idx :],
                        current_provider,
                        self.agent.name,
                    )
                    if (
                        messages_for_this_turn
                    ):  # Only save if there are messages for the turn
                        self.persistent_service.append_conversation_messages(
                            self.current_conversation_id, messages_for_this_turn
                        )
                        self._notify(
                            "conversation_saved", {"id": self.current_conversation_id}
                        )
                except Exception as e:
                    error_message = f"Failed to save conversation turn to {self.current_conversation_id}: {str(e)}"
                    print(f"ERROR: {error_message}")
                    self._notify("error", {"message": error_message})
            self.last_assisstant_response_idx = len(self.agent.history)
            # --- End of Persistence Logic ---

            return assistant_response, input_tokens, output_tokens

        except Exception as e:
            error_message = str(e)
            traceback_str = traceback.format_exc()
            self._notify(
                "error",
                {
                    "message": error_message,
                    "traceback": traceback_str,
                    "messages": self.agent.history,
                },
            )
            return None, 0, 0

    # --- Add these new methods ---
    def list_conversations(self) -> List[Dict[str, Any]]:
        """Lists available conversations from the persistence service."""
        try:
            return self.persistent_service.list_conversations()
        except Exception as e:
            print(f"Error listing conversations: {e}")
            self._notify("error", f"Failed to list conversations: {e}")
            return []

    def load_conversation(self, conversation_id: str) -> Optional[List[Dict[str, Any]]]:
        """Loads a specific conversation history and sets it as active."""
        try:
            self.agent_manager.clean_agents_messages()
            history = self.persistent_service.get_conversation_history(conversation_id)
            if history:
                self.current_conversation_id = conversation_id
                last_agent_name = history[-1].get("agent", "")
                if last_agent_name and self.agent_manager.select_agent(last_agent_name):
                    self.agent = self.agent_manager.get_current_agent()
                    self._notify("agent_changed", last_agent_name)
                current_provider = self.agent.get_provider()
                # self.messages = MessageTransformer.convert_messages(
                #     [msg for msg in history if msg.get("agent", "") == self.agent.name],
                #     current_provider,
                # )
                self.streamline_messages = history
                for message in self.streamline_messages:
                    if message.get("agent", ""):
                        agent = self.agent_manager.get_agent(message.get("agent", ""))
                        if agent:
                            agent.history.extend(
                                MessageTransformer.convert_messages(
                                    [message], current_provider
                                )
                            )

                self.last_assisstant_response_idx = len(self.agent.history)

                for i, message in enumerate(self.streamline_messages):
                    role = message.get("role")
                    if role == "user":
                        content = message.get("content", "")
                        message_content = ""

                        # Handle different content structures (standardized format)
                        if isinstance(content, str):
                            message_content = content
                        elif isinstance(content, list) and content:
                            # Assuming the first item in the list contains the primary text
                            first_item = content[0]
                            if (
                                isinstance(first_item, dict)
                                and first_item.get("type") == "text"
                            ):
                                message_content = first_item.get("text", "")
                        if (
                            message_content
                            and not message_content.startswith(
                                "Use user_context_summary to tailor your response"
                            )
                            and not message_content.startswith("Content of ")
                        ):
                            turn = ConversationTurn(message_content, i)
                            self.conversation_turns.append(turn)

                print(f"Loaded conversation {conversation_id}")  # Optional: Debugging
                self._notify(
                    "conversation_loaded", {"id": conversation_id, "history": history}
                )
                return history
            else:
                self._notify(
                    "error", f"Conversation {conversation_id} not found or empty."
                )
                return []
        except Exception as e:
            print(f"Error loading conversation {conversation_id}: {e}")
            self._notify("error", f"Failed to load conversation {conversation_id}: {e}")
